digraph {
	graph [size="22.349999999999998,22.349999999999998"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	125929684501280 [label="
 (2, 1, 128, 128)" fillcolor=darkolivegreen1]
	125929684604128 [label=ConvolutionBackward0]
	125929684603552 -> 125929684604128
	125929684603552 [label=ConvolutionBackward0]
	125929684603984 -> 125929684603552
	125929684603984 [label=ReluBackward0]
	125929692788144 -> 125929684603984
	125929692788144 [label=ConvolutionBackward0]
	125929684604320 -> 125929692788144
	125929684604320 [label=ConvolutionBackward0]
	125929684605136 -> 125929684604320
	125929684605136 [label=ReluBackward0]
	125929684603936 -> 125929684605136
	125929684603936 [label=ConvolutionBackward0]
	125929684605424 -> 125929684603936
	125929684605424 [label=ConvolutionBackward0]
	125929684605856 -> 125929684605424
	125929684605856 [label=ReluBackward0]
	125929684607392 -> 125929684605856
	125929684607392 [label=ConvolutionBackward0]
	125929684606192 -> 125929684607392
	125929684606192 [label=MaxPool2DWithIndicesBackward0]
	125929684606096 -> 125929684606192
	125929684606096 [label=ConvolutionBackward0]
	125929684606144 -> 125929684606096
	125929684606144 [label=ReluBackward0]
	125929684606672 -> 125929684606144
	125929684606672 [label=ConvolutionBackward0]
	125929684606288 -> 125929684606672
	125929684606288 [label=MaxPool2DWithIndicesBackward0]
	125929684606576 -> 125929684606288
	125929684606576 [label=ConvolutionBackward0]
	125929684606816 -> 125929684606576
	125929684606816 [label=ReluBackward0]
	125929684607056 -> 125929684606816
	125929684607056 [label=ConvolutionBackward0]
	125929684607344 -> 125929684607056
	125929684607344 [label=MaxPool2DWithIndicesBackward0]
	125929684605808 -> 125929684607344
	125929684605808 [label=ConvolutionBackward0]
	125929684607488 -> 125929684605808
	125929684607488 [label=ReluBackward0]
	125929684607968 -> 125929684607488
	125929684607968 [label=ConvolutionBackward0]
	125929684607632 -> 125929684607968
	125929684383408 [label="encoder.0.layers.0.weight
 (16, 3, 3, 3)" fillcolor=lightblue]
	125929684383408 -> 125929684607632
	125929684607632 [label=AccumulateGrad]
	125929684607824 -> 125929684607968
	125929684383488 [label="encoder.0.layers.0.bias
 (16)" fillcolor=lightblue]
	125929684383488 -> 125929684607824
	125929684607824 [label=AccumulateGrad]
	125929684607584 -> 125929684605808
	125929684383648 [label="encoder.0.layers.2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	125929684383648 -> 125929684607584
	125929684607584 [label=AccumulateGrad]
	125929684606960 -> 125929684605808
	125929684383728 [label="encoder.0.layers.2.bias
 (16)" fillcolor=lightblue]
	125929684383728 -> 125929684606960
	125929684606960 [label=AccumulateGrad]
	125929684606912 -> 125929684607056
	125929684383888 [label="encoder.2.layers.0.weight
 (32, 16, 3, 3)" fillcolor=lightblue]
	125929684383888 -> 125929684606912
	125929684606912 [label=AccumulateGrad]
	125929684606768 -> 125929684607056
	125929684383968 [label="encoder.2.layers.0.bias
 (32)" fillcolor=lightblue]
	125929684383968 -> 125929684606768
	125929684606768 [label=AccumulateGrad]
	125929684607008 -> 125929684606576
	125929684384128 [label="encoder.2.layers.2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	125929684384128 -> 125929684607008
	125929684607008 [label=AccumulateGrad]
	125929684606864 -> 125929684606576
	125929684384208 [label="encoder.2.layers.2.bias
 (32)" fillcolor=lightblue]
	125929684384208 -> 125929684606864
	125929684606864 [label=AccumulateGrad]
	125929684606480 -> 125929684606672
	125929684384368 [label="encoder.4.layers.0.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
	125929684384368 -> 125929684606480
	125929684606480 [label=AccumulateGrad]
	125929684606432 -> 125929684606672
	125929684384448 [label="encoder.4.layers.0.bias
 (64)" fillcolor=lightblue]
	125929684384448 -> 125929684606432
	125929684606432 [label=AccumulateGrad]
	125929684605952 -> 125929684606096
	125929684384608 [label="encoder.4.layers.2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	125929684384608 -> 125929684605952
	125929684605952 [label=AccumulateGrad]
	125929684605760 -> 125929684606096
	125929684384688 [label="encoder.4.layers.2.bias
 (64)" fillcolor=lightblue]
	125929684384688 -> 125929684605760
	125929684605760 [label=AccumulateGrad]
	125929684604800 -> 125929684607392
	125929684384848 [label="decoder.0.layers.0.weight
 (64, 32, 2, 2)" fillcolor=lightblue]
	125929684384848 -> 125929684604800
	125929684604800 [label=AccumulateGrad]
	125929684605568 -> 125929684607392
	125929684384928 [label="decoder.0.layers.0.bias
 (32)" fillcolor=lightblue]
	125929684384928 -> 125929684605568
	125929684605568 [label=AccumulateGrad]
	125929684605040 -> 125929684605424
	125929684385088 [label="decoder.0.layers.2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	125929684385088 -> 125929684605040
	125929684605040 [label=AccumulateGrad]
	125929684609120 -> 125929684605424
	125929684385168 [label="decoder.0.layers.2.bias
 (32)" fillcolor=lightblue]
	125929684385168 -> 125929684609120
	125929684609120 [label=AccumulateGrad]
	125929684605520 -> 125929684603936
	125929684385328 [label="decoder.1.layers.0.weight
 (32, 16, 2, 2)" fillcolor=lightblue]
	125929684385328 -> 125929684605520
	125929684605520 [label=AccumulateGrad]
	125929684605232 -> 125929684603936
	125929684385408 [label="decoder.1.layers.0.bias
 (16)" fillcolor=lightblue]
	125929684385408 -> 125929684605232
	125929684605232 [label=AccumulateGrad]
	125929684604944 -> 125929684604320
	125929684385568 [label="decoder.1.layers.2.weight
 (16, 16, 3, 3)" fillcolor=lightblue]
	125929684385568 -> 125929684604944
	125929684604944 [label=AccumulateGrad]
	125929684605184 -> 125929684604320
	125929684385648 [label="decoder.1.layers.2.bias
 (16)" fillcolor=lightblue]
	125929684385648 -> 125929684605184
	125929684605184 [label=AccumulateGrad]
	125929684604896 -> 125929692788144
	125929684500560 [label="decoder.2.layers.0.weight
 (16, 8, 2, 2)" fillcolor=lightblue]
	125929684500560 -> 125929684604896
	125929684604896 [label=AccumulateGrad]
	125929684601488 -> 125929692788144
	125929684500640 [label="decoder.2.layers.0.bias
 (8)" fillcolor=lightblue]
	125929684500640 -> 125929684601488
	125929684601488 [label=AccumulateGrad]
	125929684603648 -> 125929684603552
	125929684500800 [label="decoder.2.layers.2.weight
 (8, 8, 3, 3)" fillcolor=lightblue]
	125929684500800 -> 125929684603648
	125929684603648 [label=AccumulateGrad]
	125929684604752 -> 125929684603552
	125929684500880 [label="decoder.2.layers.2.bias
 (8)" fillcolor=lightblue]
	125929684500880 -> 125929684604752
	125929684604752 [label=AccumulateGrad]
	125929684604368 -> 125929684604128
	125929684501040 [label="conv.weight
 (1, 8, 1, 1)" fillcolor=lightblue]
	125929684501040 -> 125929684604368
	125929684604368 [label=AccumulateGrad]
	125929684604224 -> 125929684604128
	125929684501120 [label="conv.bias
 (1)" fillcolor=lightblue]
	125929684501120 -> 125929684604224
	125929684604224 [label=AccumulateGrad]
	125929684604128 -> 125929684501280
}
